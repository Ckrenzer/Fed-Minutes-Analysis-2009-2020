---
title: "Minutes Analysis"
author: "Connor Krenzer"
date: "1/4/2021"
output:
  pdf_document: default
  toc: yes
  toc_depth: 2
editor_options: 
  chunk_output_type: console
---
## Introduction
The Federal Reserve is the central bank of the United States. Central banks conduct a country's monetary policy (policy pertaining to money--interest rates, regulation of the banking sector, etc.) in the aims of achieving it's 'dual mandate' of full employment (loosely defined as "anyone who wants a job can get a job") and price stability (predictable price changes throughout the United States as a whole). You can read more about what the Fed is and what they do [on their website](https://www.federalreserve.gov/faqs/about_12594.htm). For all of you Fed haters who think I'm misleading the public by linking to the Fed's website, do not disparage! Perhaps [one of these articles](https://mises.org/topics/fed) is more your style.





Every six weeks, policy makers on the Federal Reserve's Federal Open Market Committee (FOMC) meet to discuss economic indicators to determine and vote on policy in response to these indicators, among other things. The FOMC Statement released following this meeting can send the entire economy into turmoil--so much so that the minutes are withheld from the public for three weeks before becoming available.





The data in this analysis includes all FOMC Minutes between January 28th, 2009 to November 5th, 2020 (inclusive). As of the time of writing, the Minutes for the December 15th-16th, 2020 meeting have not been released.

$~$

I will explore these texts using various text mining approaches including sentiment analysis, tf-idf, and topic modeling to identify trends within the FOMC's Meetings.


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      results = "hide",
                      error = F,
                      warning = F,
                      message = F)
```


## Packages

I recommend pulling the information using the HTML version of these meetings. The PDFs are a nightmare to clean (speaking from experience)! If you do decide to read the pdf text into R, I suggest using the extract_text() function from the tabulizer package because it is smart enough to recognize columns of selectable PDF text. Other packages popular for reading in PDFs, such as readtext or pdftools, will only read across the page left to right, then down--despite the pages containing two separate columns of text.


```{r packages, echo = FALSE}
# The names of all your installed packages
package_names <- rownames(installed.packages())

# Installing packages used in this project
if(!"dplyr" %in% package_names) install.packages("dplyr")
if(!"readr" %in% package_names) install.packages("readr")
if(!"stringr" %in% package_names) install.packages("stringr")
if(!"tidytext" %in% package_names) install.packages("tidytext")
if(!"rvest" %in% package_names) install.packages("rvest")
if(!"tidyr" %in% package_names) install.packages("tidyr")
if(!"ggplot2" %in% package_names) install.packages("ggplot2")
if(!"patchwork" %in% package_names) install.packages("patchwork")
if(!"lubridate" %in% package_names) install.packages("lubridate")
if(!"knitr" %in% package_names) install.packages("knitr")
if(!"igraph" %in% package_names) install.packages("igraph")
if(!"ggraph" %in% package_names) install.packages("ggraph")
if(!"grid" %in% package_names) install.packages("grid")
if(!"widyr" %in% package_names) install.packages("widyr")
if(!"topicmodels" %in% package_names) install.packages("topicmodels")
if(!"textdata" %in% package_names) install.packages("textdata")

# Loading in dplyr because it is used too often
# to :: it the entire time
library(dplyr)

# Packages which are difficult to use without loading:
library(rvest)
library(ggplot2)
library(patchwork)

```


## Data Import

Since I started this process using the pdf files, I will use their file names to determine the corresponding url. If I had started mining this data from HTML in the first place, a more elegant solution might have been used, but downloading all the PDF files and using their names to put together urls works fine enough...

There are two formats for the HTML file--one for the Fed Minutes before 2012 and those released on or after that year. With this in mind, two functions will be used to read in the different formats. The documents are parsed in chronological order but are also arranged after being added to the tibble, in the event that something were to happen to the order of the files in the "Fed Minutes Releases" folder from which these names were pulled.


###### Note: As of 6/26/2021, the web scraper no longer pulls all the information off the Federal Reserve's website each time this document is knitted. Instead, the results were stored in a file that gets read in to ensure reproducibility (should anything on the Fed's website change in the future).

```{r import, echo = FALSE, eval = FALSE}

# The code in this section "makes" the dataset
old_format <- function(id){
  
  # Note: using str_c() to prevent the code from going out of
  # bounds in the output (so I don't have to get into dark LaTex magic!)
  name <- stringr::str_c("https://www.federalreserve.gov/monetarypolicy/",
                         id,
                         ".htm")
  
  tibble(doc_id = id, text = read_html(name) %>% 
           html_nodes("#leftText") %>% 
           html_text() %>% 
           stringr::str_split("\n", simplify = T) %>% 
           as.vector() %>% 
           stringr::str_to_lower() %>% 
           stringr::str_replace_all("'s|\"|,|:|;|-{2,}", " ") %>% 
           magrittr::extract(!stringr::str_detect(., "_+")) %>%  # removing the signature
           stringr::str_c(., collapse = " ") %>%
           stringr::str_replace_all("mr\\.{1}|mrs\\.{1}|ms\\.{1}", " ") %>% 
           stringr::str_replace_all("messrs[\\.]?|mmes[\\.]?", " ") %>% 
           stringr::str_replace_all("mss[\\.]?|mses[\\.]?", " ") %>% 
           stringr::str_replace_all("a\\.m\\.|p\\.m\\.|d\\.c\\.", " ") %>% 
           stringr::str_replace_all("\\d", " ") %>% 
           stringr::str_squish() %>% 
           stringr::str_remove_all("return to text[\\d]*|return to top") %>% 
           stringr::str_trim()
  ) %>% 
    return()
  
}#end of old_format()


# For releases 2012 and onward:
# We can always assume the first 37 entries are bogus
# Keep everything following the 37th entry
new_format <- function(id){
  name <- stringr::str_c("https://www.federalreserve.gov/monetarypolicy/",
                         id,
                         ".htm")
  
  tibble(doc_id = id, text = read_html(name) %>% 
           html_nodes("p") %>% 
           html_text() %>% 
           stringr::str_trim() %>% 
           stringr::str_squish() %>% 
           magrittr::extract(38:length(.)) %>% 
           magrittr::extract(!stringr::str_detect(., "_+")) %>%  # removing the signature
           stringr::str_to_lower() %>% 
           stringr::str_replace_all("'s|\"|,|:|;|-{2,}", " ") %>% 
           stringr::str_c(., collapse = " ") %>%
           stringr::str_replace_all("mr\\.{1}|mrs\\.{1}|ms\\.{1}", " ") %>% 
           stringr::str_replace_all("messrs[\\.]?|mmes[\\.]?", " ") %>% 
           stringr::str_replace_all("mss[\\.]?|mses[\\.]?", " ") %>% 
           stringr::str_replace_all("a\\.m\\.|p\\.m\\.|d\\.c\\.", " ") %>% 
           stringr::str_replace_all("\\d", " ") %>% 
           stringr::str_squish() %>% 
           stringr::str_replace_all("return to text[\\d]*|return to top", " ") %>% 
           stringr::str_trim()
  ) %>% 
    return()
  
}#end of new_format()






# The paths to the PDF files
dirs <- c("Fed Minutes Releases/Obama Era/Pre 2012/",
          "Fed Minutes Releases/Obama Era/Post 2012/",
          "Fed Minutes Releases/Trump Era/")


# An empty tibble--document names and their corresponding
# text will be added to it.
documents <- tibble::tibble(doc_id = NULL, text = NULL)


# I'm not crazy about for loops, but I think this code is
# straightforward enough to follow
# Reading in the old formats (pre-2012)
for(i in stringr::str_remove_all(list.files(dirs[1]), "\\.pdf")){
  documents <- bind_rows(documents, old_format(i))
  
  # simple yet effective way of showing the operation's progress
  cat(".")
}

# Reading in the new formats (2012-present)
for(i in stringr::str_remove_all(list.files(dirs[2:3]), "\\.pdf")){
  documents <- bind_rows(documents, new_format(i))
  
  # simple yet effective way of showing the operation's progress
  cat(".")
}

# Arranging the documents in chronological order
documents <- arrange(documents, doc_id)

```


```{r loading in data, echo = FALSE, include = FALSE}

# This file contains the results from the above chunk at the time of writing
documents <- readr::read_rds(file = "fomc_minutes_text.rds")

```



## The Dataset
The data is almost ready for analysis. We now have two columns: one for the document name, and another for the contents of that document. If you are interested in performing a similar analysis, the data frame can now be transformed in a number of ways. You can use the unnest_tokens() function from the tidytext package to extract tokens (words, groups of words, paragraphs, etc.) and perform sentiment analysis, or you could make a term-document matrix and perform topic modeling.

We will do both, but let's begin by taking a look at the dataset:


> `r stringr::str_sub(documents$text[1], start = 8482, end = 8802)`


The quote above is an excerpt from the Minutes release for the meeting dated January 28th, 2009. You might notice that some words have parsing irregularities, such as the lack of space between characters on the parentheses surrounding "millions of dollars equivalent." This section of text was chosen to represent the 'worst-case-scenario' for parsing text. No algorithm is perfect, but one can see these errors are infrequent and therefore unlikely to heavily skew the results of the analysis. The text is from a table detailing relations with the Bank of Canada and the Bank of Mexico--tables typically have issues parsing.

$~$

A great place to start any textual analysis is with word counts.



## Common Words

Brian F. Madigan is the Secretary who writes all these meetings--he has been the secretary for at least the last 12 years! In a way, we are exploring one man's work over the course of a decade...

After removing 'stop words'--those words that are extremely common and provide very little meaning (Ex. "the," "of," "a," "to," etc.)--which words does Brian use most frequently?


```{r all docs, echo = FALSE, results = "show"}
unigram_unnested_documents <- documents %>% 
  tidytext::unnest_tokens(word, text)



head(unigram_unnested_documents %>% 
       anti_join(tidytext::stop_words) %>% 
       count(word, sort = T), 10) %>% 
  ggplot(aes(x = reorder(word, n), y = n, fill = word)) +
  geom_col(show.legend = F) +
  xlab(NULL) +
  ylab("# of Uses") +
  ggtitle("# of Uses Across All Documents") +
  coord_flip() +
  theme_bw()

```


These are words one expects to find in official banking documents; the terms "committee," "inflation," and "market" dominate across all `r nrow(documents)` meetings. They may be insightful to some, but most people would agree these terms are uninteresting finds. Much of what Brian wrote is generic banking lingo from which little meaning can be derived. We can still use these terms to draw interesting conclusions, however; we will visualize their usage over time, for example, but we first want to know which terms are unique to specific documents.

To extract words that have most significance in each document, we can add a weighting factor to terms with the tf-idf statistic.








## TF-IDF

The tf-idf statistic is designed to reduce the importance of words that occur often across documents. The math behind it is based on the natural logarithm. It is the product of the 'term frequency' and 'inverse document frequency':

_Term Frequency = (# of Occurrences in Document) / (# of Unique Words in Document)_

_Inverse Document Frequency = ln((Total # of Docs in Corpus) / (# of Docs where Term Appears))_

Therefore...
__tf-idf = (tf * idf)__


$~$

You can think of tf-idf as a formula that finds terms that occur frequently--but not too frequently. Let's see the most important words among Fed Minutes releases, as measured by the tf-idf:


```{r tf-idf table, echo = FALSE, results = "show"}

knitr::kable(head(unigram_unnested_documents %>% 
                    count(doc_id, word, sort = T) %>%
                    ungroup() %>% 
                    tidytext::bind_tf_idf(word, doc_id, n) %>% 
                    arrange(desc(tf_idf)), 10),
             caption = "Important Words as Measured by tf-idf")

```


I'm tired of seeing those words everywhere. The correct interpretation of this table, however, is that "pandemic," "coronavirus," and "outbreak" are the most important terms, as determined by the tf-idf. Let's filter out the words "pandemic," "coronavirus," "virus," and "outbreak" to see what else tf-idf deems important. Finally, after some inspection, the names of the months, "shall," and "our" should also be filtered out.

Let's see a plot of these terms after the removal:


```{r tf-idf graph, echo = FALSE, results = "show"}

unigram_unnested_documents %>% 
  count(doc_id, word, sort = T) %>%
  ungroup() %>% 
  filter(!word %in% c("pandemic", "coronavirus", "virus",
                      "outbreak", "our", "shall",
                      stringr::str_to_lower(month.name))) %>% 
  tidytext::bind_tf_idf(word, doc_id, n) %>% 
  arrange(desc(tf_idf)) %>% 
  slice(1:10) %>% 
  ggplot(aes(x = reorder(word, tf_idf), y = tf_idf, fill = doc_id)) +
  geom_col(show.legend = F) +
  xlab(NULL) +
  ylab("tf-idf") +
  ggtitle("TF-IDF Statistic for Fed Minutes") +
  coord_flip() +
  theme_bw()

```


That's better. The Fed uses many different acronyms to describe its programs. To name a few, "yct" is the "yield control target," talf is short for "term asset-backed securities loan facilities," and "elb" stands for "effective lower bound." In terms of weather, 2017 was a tough year for the Atlantic and Gulf Coasts of the United States--so much so that "hurricanes" (Hurricanes Harvey, Irma, and Maria formed in 2017--the FOMC mentioned Harvey and Irma primarily) took two of the top 10 slots. [The Fed took measures to provide relief in the aftermath of the storms](https://www.federalreserve.gov/supervisionreg/hurricane-harvey.htm), which explains the word's prominence.

$~$

Aside from the obvious, you may be wondering how these terms relate to the current events at the time. Yield curve controls (yield curve targets, YCT), for instance, were a hot topic in June 2020, when the Fed evaluated the experiences of central banks across the developed world in controlling government bond yields in response to the recession. As a matter of fact, "yct" only appeared in _one_ Fed Minutes release and appeared fifteen times! The second phrase, term asset-backed securities loan facilities (talf), was a major policy the Fed pursued in the aftermath of the Great Recession.

Clearly, then, we can see the value brought from this statistic. It allows us to identify important aspects of documents to hone in on topics of interest.













## Change Over Time

In addition to finding which terms are important to specific documents (FOMC Meetings in this instance), we can also calculate the usage of words over time. We can find some cool trends, such as the following:


```{r change over time, echo = FALSE, results = "show", fig.height = 10}

change_over_time <- function(data, word1, word2, graph_title){
  
  graph_title <-  paste("Usage of \"",
                        stringr::str_to_title(word1),
                        "\" and \"",
                        stringr::str_to_title(word2),
                        "\" Over Time",
                        sep = "")
  
  data %>% 
    count(doc_id, word) %>% 
    ungroup() %>% 
    filter(word %in% c(word1, word2)) %>% 
    mutate(doc_id = lubridate::as_date(stringr::str_sub(doc_id, start = 12))) %>% 
    ggplot(aes(x = doc_id, y = n, color = word)) +
    geom_line(size = 1.3, show.legend = T) +
    xlab("FOMC Meetings") +
    ylab("# of Uses") +
    ggtitle(graph_title) +
    theme_bw()
  
}#end of change_over_time()

# These plots are put on top of one another using the 
# patchwork package's "/" operator
change_over_time(data = unigram_unnested_documents,
                 word1 = "employment",
                 word2 = "inflation") /
  
  change_over_time(data = unigram_unnested_documents,
                   word1 = "dollar",
                   word2 = "china") /
  
  change_over_time(data = unigram_unnested_documents,
                   word1 = "trade",
                   word2 = "job") /
  
  change_over_time(data = unigram_unnested_documents,
                   word1 = "stability",
                   word2 = "inflation") /
  
  change_over_time(data = unigram_unnested_documents,
                   word1 = "gdp",
                   word2 = "wage")


```


Of course, we don't have to restrict our comparisons of words used in FOMC meetings to other words in FOMC meetings. Why don't we compare the Fed's use of "inflation" with 10 year bond yields? 

###### Note: The words "employment" and "unemployment" have similar usage over time, so I picked "employment" for the graph. Their numbers could be pooled, but the purpose of these graphs is only to introduce ways of finding trends in the dataset.


###### Note: Only a handful of FOMC meetings started on the first day of the month in the sample period, so the index's base will be set to 100 on November 1st, 2017. Most economic data only comes out monthly, quarterly, etc., and a common base is needed for comparison.


```{r treasury maturities, echo = FALSE, results = "show"}

bonds <- readr::read_csv("10 Yr Treasury Constant Maturity Rate Index.csv",
                         col_names = T)
# The csv file had a period where missing values go
bonds$ind[which(bonds$ind == ".")] <- NA

# Setting the data to numeric
bonds$ind <- as.numeric(bonds$ind)



# inflation-mention index
inf_mentions <- unigram_unnested_documents %>% 
  count(doc_id, word) %>% 
  ungroup() %>% 
  filter(word == "inflation") %>% 
  mutate(doc_id = lubridate::as_date(stringr::str_sub(doc_id, start = 12))) %>% 
  mutate(`n` = (unlist(.[, 3]) / unlist(.[which(unlist(.$doc_id) == "2017-11-01"), 3])) * 100)



# The line chart
ggplot() +
  geom_line(data = inf_mentions,
            aes(x = doc_id, y = n, color = "infl"),
            size = 1.02) +
  geom_line(data = bonds,
            aes(x = date, y = ind, color = "bond"),
            size = 1.02) +
  xlab("Date") +
  ylab("Index (Base: Nov 1, 2017)") +
  ggtitle("FOMC Mentions of \"Inflation\" vs. 10 Year Treasury Maturity Rate") +
  theme_bw()

```


As we can see, FOMC mentions of inflation and the constant maturity rate seem to be inversely related.



## N-GRAMS

We do not need to restrict our analysis to one word. We can extract multiple words, or "n-grams," to add context to the phrases the Fed uses. We can do the same analysis as in the previous sections with two or three words instead. After some investigation, know that the words "federal," "reserve," "committee," and "intermeeting" are also removed, as I suspect they mostly refer to formalities within the documents and not necessarily to the topics discussed at FOMC meetings.

First, the most common pairs of words:


```{r bigram uses graph, echo = FALSE, results = "show"}

# The bigrams
bigram_unnested_documents <- documents %>% 
  tidytext::unnest_tokens(word, text, token = "ngrams", n = 2)


bigram_unnested_documents_filtered <- bigram_unnested_documents %>% 
  tidyr::separate(col = "word", into = c("word1", "word2")) %>% 
  filter(!word1 %in% tidytext::stop_words$word,
         !word1 %in% c("federal", "reserve", "committee", "intermeeting"),
         !word2 %in% tidytext::stop_words$word,
         !word2 %in% c("federal", "reserve", "committee", "intermeeting")) %>% 
  tidyr::unite(col = "word", c("word1", "word2"), sep = " ")


# The bar chart for the bigrams
bigram_unnested_documents_filtered %>% 
  count(word, sort = TRUE) %>% 
  slice(1:10) %>% 
  ggplot(aes(x = reorder(word, n), y = n, fill = word)) +
  geom_col(show.legend = F)+
  xlab(NULL) +
  ylab("# of Uses") +
  ggtitle("# of Uses Across All Documents") +
  coord_flip() +
  theme_bw()


```


Using two words for the analysis allows us to ask more specific questions about the data that could not be answered with just one word. It allows us to ask things like, how has the Fed's use of "financial markets" compared with its use of "labor market" over time?


```{r finc vs labor, echo = FALSE, results = "show"}


# Usage of "financial markets" vs. "labor market" over time
bigram_unnested_documents_filtered %>% 
  count(doc_id, word, sort = TRUE) %>% 
  filter(word %in% c("financial markets", "labor market")) %>% 
  mutate(doc_id = lubridate::as_date(stringr::str_sub(doc_id, start = 12))) %>% 
  tidyr::pivot_wider(names_from = word, values_from = n) %>%
  mutate(`labor market` = (unlist(.[, 2]) / unlist(.[which(unlist(.$doc_id) == "2017-11-01"), 2])) * 100,
         `financial markets` = (unlist(.[, 3]) / unlist(.[which(unlist(.$doc_id) == "2017-11-01"), 3])) * 100) %>% 
  tidyr::pivot_longer(cols = c(`financial markets`, `labor market`), names_to = "word", values_to = "ind") %>% 
  mutate(word = recode(word,
                       `financial markets` = "finc",
                       `labor market` = "labor")) %>% 
  ggplot(aes(x = doc_id, y = ind, color = word)) +
  geom_line(size = 1.02) +
  xlab("Date") +
  ylab("Index (Base: Nov 1, 2017)") +
  ggtitle("FOMC Mentions of \"Financial Markets\" vs. \"Labor Market\"") +
  theme_bw()

```


We can extend this line of thinking further with pairings of three words:


```{r trigram uses graph, echo = FALSE, results = "show"}


# The trigrams bar chart
documents %>% 
  tidytext::unnest_tokens(word, text, token = "ngrams", n = 3) %>% 
  tidyr::separate(col = "word", into = c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% tidytext::stop_words$word,
         !word1 %in% c("federal", "reserve", "committee", "intermeeting"),
         !word2 %in% tidytext::stop_words$word,
         !word2 %in% c("federal", "reserve", "committee", "intermeeting"),
         !word3 %in% tidytext::stop_words$word,
         !word3 %in% c("federal", "reserve", "committee", "intermeeting"),) %>% 
  tidyr::unite(col = "word", c("word1", "word2", "word3"), sep = " ")%>% 
  count(word, sort = TRUE) %>% 
  slice(1:10) %>% 
  ggplot(aes(x = reorder(word, n), y = n, fill = word)) +
  geom_col(show.legend = F)+
  xlab(NULL) +
  ylab("# of Uses") +
  ggtitle("# of Uses Across All Documents") +
  coord_flip() +
  theme_bw()

```


These terms might benefit from exploring combinations of four to find words that precede these phrases, such as "_strong_ labor market conditions" or "_long_ term inflation expectations." This is not an optimal path to pursue, however, because the more combinations of terms there are to find, the fewer matches in the data there will be. Going from two to three terms made the "# of Uses" drop by about one-third. We can find these kinds of patterns another way. As we will soon see, networks find most of these combinations for us.







## Sentiment Analysis


Now that we have terms stored as bigrams (and trigrams!), we can perform sentiment analysis on the Fed Minutes releases. Various lexicons have been built up over the years to determine the sentiments of a document. In this case, the Loughran and McDonald dictionary of financial sentiment words (Loughran and McDonald 2011) will be used. Other popular sentiment lexicons, such as the "AFINN," "NRC," or "BING" are not suited for documents coming from a bank--these three lexicons assign values to terms such as "share" or "risk" despite the fact those words may not carry any connotation in a financial setting.

Sentiment analysis could be used for the terms without sorting them into pairs, but this adds error when negation terms are used. For example, _weak_ growth does not mean the same thing as _strong_ growth, but this distinction would never be found by using only one term (the word "growth" does not appear in the loughran lexicon).

Whenever negation words preface a word in the loughran lexicon, we can flip its sign or set its sentiment value to zero, ensuring negation words do not skew the true sentiment values.

Before continuing to the sentiment analysis, it is important to note that sentiment lexicons do not contain the same number of positive words as negative words--there is an imbalance (in most cases, sentiment lexicons have more positive words than negative words--Loughran and McDonald's lexicon is no different). The results, then, should be considered in relation to one another rather than in an absolute sense. For comparisons to other variables, an index is likely more appropriate.


```{r sentiment analysis, echo = FALSE, results = "show"}

negation_words <- c("not", "no", "weak", "low", "never", "without", "slow")

# counts, then puts each word in its own column,
# then formats the date in `doc_id` and searches for negations
# (which will negate the value of `n` if TRUE),
# then joins the sentiment lexicon values,
# then filters for only the positive and negative sentiments,
# then negates values of `n` with a negative sentiment,
# then takes the sum of the values of `n` on each date,
# then plots
bigram_unnested_documents %>% 
  count(doc_id, word) %>%
  tidyr::separate(col = word,
                  into = c("word1", "word2"),
                  sep = " ") %>%  
  mutate(doc_id = lubridate::as_date(stringr::str_sub(doc_id, start = 12)),
         n = ifelse(word1 %in% negation_words, -n, n)) %>% 
  inner_join(tidytext::get_sentiments("loughran"),
             by = c(word2 = "word")) %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  tidyr::pivot_wider(names_from = sentiment,
                     values_from = n,
                     values_fill = 0) %>% 
  group_by(doc_id) %>% 
  summarize(sentiment = sum(positive) - sum(negative)) %>% 
  ggplot(aes(x = doc_id, y = sentiment, color = "red")) +
  geom_line(show.legend = F, size = 1.02) +
  xlab("Date") +
  ylab("Sentiment Value") +
  ggtitle("Sentiment of FOMC Minutes using Loughran and McDonald Lexicon") +
  theme_bw()

```


Look at that dip at the beginning of the 2020 recession! They don't call economics the dismal science for nothing...










## Markov Chains

To visualize the relationship between the words further, we can arrange groups of words into a network.

```{r bigram markov chain, echo = FALSE, results = "show", fig.height = 12.5, fig.width = 11}

set.seed(2021)
ggraph::ggraph(bigram_unnested_documents_filtered %>% 
                 tidyr::separate(col = word,
                                 into = c("word1", "word2"),
                                 sep = " ") %>% 
                 count(word1, word2, sort = T) %>% 
                 filter(n > 150) %>% 
                 igraph::graph_from_data_frame(),
               layout = "fr") +
  ggraph::geom_edge_link(aes(edge_alpha = n), show.legend = F,
                         end_cap = ggraph::circle(.07, "inches"),
                         arrow = grid::arrow(type = "closed",
                                             length = unit(.15, "inches"))) +
  ggraph::geom_node_point(color = "lightblue", size = 5) +
  ggraph::geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  ggraph::theme_graph() 

```

We see how the words relate to one another. Many are interconnected in the massive web going down the left side, but other networks are also formed for separate groups of associated words. While terms such as "participants," "market," "economic," or "inflation" dominate the large network, other networks exist and are largely independent of these words, such as "job gains" or "forward guidance."

While this visualization of a Markov Chain allows us to visualize which words are typically follow one another, it does not show which words are related within documents. What do we do when we want to see which words appear in the same document (though not necessarily written adjacent to one another)?

A correlation test addresses this issue.






## Correlated Words

We can calculate Pearson correlation coefficients to determine how often particular words appear with one another. Examining correlation coefficients across each of the FOMC Meetings is not ideal since each one consists of thousands of words, many of them being used in each document. The most highly correlated words, broken down by document as opposed to by another tokenization, _that are not perfectly correlated_ are still provided below, however. Many of the documents are more than ten pages long and refer to similar topics, so many of the words will have perfect correlation.

```{r correlation all docs, echo = FALSE, results = "show"}

knitr::kable(unigram_unnested_documents %>% 
               anti_join(tidytext::stop_words, by = "word") %>% 
               count(doc_id, word) %>%
               filter(n > 95) %>% 
               group_by(word) %>% 
               widyr::pairwise_cor(word, doc_id, sort = T) %>%
               ungroup() %>% 
               slice_max(order_by = abs(correlation), n = 20) %>% 
               slice(seq(1, 20, by = 2)),
             digits = 3,
             caption = "Phi Coefficients for Whole Documents")



```


As implied above, what should instead be done is break the documents into smaller pieces and calculate correlation coefficients for each of these subsections. To ensure proper nouns do not dominate the results with perfect correlation (and because R said my computer ran out of memory =/), only those terms which appear more than 100 times will be used to calculate the coefficients. Also note that a significant number of words were removed--words such as "secretary," "jerome," "tuesday," "dallas," "governors," and many similar terms--to keep only the information we are interested in.


```{r correlation doc sections, echo = FALSE, results = "show"}

parsing_errors_and_formalities <- c("secretary",
                                    "counsel",
                                    "chairman",
                                    "assistant",
                                    "director",
                                    "session",
                                    "attended",
                                    "board",
                                    "governors",
                                    "associate",
                                    "division",
                                    "office",
                                    "jerome",
                                    "powell",
                                    "john",
                                    "williams",
                                    "charles",
                                    "evans",
                                    "bullard",
                                    "janet",
                                    "yellen",
                                    "george",
                                    "esther",
                                    "loretta",
                                    "mester",
                                    "jeffrey",
                                    "lacker",
                                    "tarullo",
                                    "smith",
                                    "rosengren",
                                    "michael",
                                    "william",
                                    "david",
                                    "matthew",
                                    "robert",
                                    "james",
                                    "daniel",
                                    "christopher",
                                    "mark", ######### mark might not refer to a person...
                                    "eric",
                                    "kansas",
                                    "philadelphia",
                                    "richmond",
                                    "atlanta",
                                    "york",
                                    "san",
                                    "francisco",
                                    "minneapolis",
                                    "dallas",
                                    "louis",
                                    "boston",
                                    "brainard",
                                    "sunday",
                                    "monday",
                                    "tuesday",
                                    "wednesday",
                                    "thursday",
                                    "friday",
                                    "saturday",
                                    "january",
                                    "february",
                                    "march",
                                    "april",
                                    "may",
                                    "june",
                                    "july",
                                    "august",
                                    "september",
                                    "october",
                                    "november",
                                    "december",
                                    "city",
                                    "st")

# Table of highly correlated terms
corr <- documents %>% 
  select(-doc_id) %>% 
  tidytext::unnest_paragraphs(sentence, text, paragraph_break = ".") %>% 
  filter(stringr::str_length(sentence) > 10) %>%
  mutate(section = row_number() %/% 10) %>% 
  tidytext::unnest_tokens(word, sentence) %>% 
  filter(!word %in% tidytext::stop_words$word,
         !word %in% parsing_errors_and_formalities) %>% 
  group_by(word) %>% 
  filter(n() > 100) %>% 
  widyr::pairwise_cor(word, section, sort = T)


knitr::kable(corr %>% 
               slice(seq(2, 20, by = 2)) %>% 
               rename("Word 1" = item1, "Word 2" = item2, "Correlation" = correlation),
             digits = 3,
             caption = "Phi Coefficients for Document Sections")



```

The remainder of this section uses data from the above table.

$~$

Let's see a nice graph showing the top 10 correlations of six words:


```{r correlation for chosen words, echo = FALSE, results = "show"}

corr %>% 
  filter(item1 %in% c("markets", "money", "regulation", "force", "statistics", "family")) %>% 
  group_by(item1) %>% 
  slice_max(abs(correlation), n = 10) %>% 
  ungroup() %>% 
  ggplot(aes(reorder(item2, correlation), correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() +
  xlab(NULL) +
  ylab("Correlation") +
  ggtitle("Most Correlated Terms for Each Word") +
  theme_bw()

```

Pay attention to the scale and note that it is not the same for each of the six terms. These graphs paint a similar picture to the graphs from the N-grams section, though this one is a bit more telling because even though words that frequently appear next to each other are still included (Ex. "[labor] force participation"), words that often occur in the same sentence or set of sentences appear as well (Ex. "family" and "disposable" are unlikely to be written after one another in a document--which is the only way for n-grams to find a connection between the two--though correlation coefficients still recognize such co-occurrences).

One of the best uses of these correlations is to pick interesting words and find how other terms relate to them in a network, similar to that from the previous section. Note that because there is no directional relationship, there are no arrows on the graph. Since the number of words is enormous, only those terms with a correlation of at least 0.6 are included.


```{r correlation network, echo = FALSE, results = "show", fig.height = 10.5, fig.width = 11}

set.seed(2021)
corr %>% 
  filter(correlation > .6) %>% 
  igraph::graph_from_data_frame() %>% 
  ggraph::ggraph(layout = "fr") +
  ggraph::geom_edge_link(aes(edge_alpha = correlation), show.legend = F) +
  ggraph::geom_node_point(color = "lightblue", size = 5) +
  ggraph::geom_node_text(aes(label = name), repel = T) +
  ggraph::theme_graph()

```


As noted, this graph is very similar to the one from the Markov Chains section. The difference between this graph and its counterpart follows the same explanation provided moments before: n-grams 'pick up' fewer relationships between terms than do correlation coefficients. Correlation coefficients capture relationships between all words, not only those found in succession. One shortcoming of this graph compared to the one from the previous section is that this graph cannot contain directional arrows, so we are unable to tell whether words were used in conjunction with one another, or whether they merely relate to similar topics.






## Topic Modeling

Speaking of topics, we can use algorithms to identify topics within the dataset. I will be using the latent Dirichlet allocation algorithm to identify these topics. LDA captures document-topic probabilities (with document defined loosely as "a group of words"). Every document is a mixture of topics, and every topic is a mixture of words. For example, let's say that the topics in the FOMC Minutes from January 28th, 2009 are be 40% inflation, 30% financial markets, and 30% labor markets (_every document is a mixture of topics_). The inflation topic could be made up of words such as "rate," "cpi," etc., while the financial markets topic could be made up of terms such as "rate," "yield," etc. (_every topic is a mixture of words_).

You might have noticed that the word "rate" appeared in both the inflation and financial markets topics. That is fine--unlike other unsupervised learning/clustering methods, LDA allows words to be reused in different topics. Just as two words have different meanings in human language, two words can appear in different topics with LDA.

```{r lda and common words per topic, echo = FALSE, results = "show", fig.height = 6}

# This took around 3 min 45 sec to run
fed_lda <- unigram_unnested_documents %>% 
  count(doc_id, word) %>% 
  anti_join(tidytext::stop_words, by = "word") %>% 
  filter(!word %in% parsing_errors_and_formalities) %>% 
  tidytext::cast_dtm(doc_id, word, n) %>% 
  topicmodels::LDA(k = 8, control = list(seed = 2021))


# Extracts the per-topic-per-word probabilities ("beta") from the model
# "aaa" has a 0.000116 probability being generated from topic 3.
fed_topics_beta <- tidytext::tidy(fed_lda, matrix = "beta")


# A ggplot of the most common words in each topic
fed_topics_beta %>% 
  group_by(topic) %>% 
  slice_max(order_by = beta, n = 7) %>% 
  ungroup() %>% 
  arrange(topic, -beta) %>% 
  mutate(topic = paste0("Topic ", topic)) %>% 
  mutate(term = reorder(term, beta)) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~ topic, scales = "free", ncol = 2) +
  theme_bw() +
  coord_flip()

```

There is much overlap in the topics the model found. This is not surprising--the FOMC usually has a predictable agenda it needs to get through.

We may instead be interested in those words having the greatest difference between groups (as opposed to those most likely to appear in a particular topic). We can use a log ratio to calculate this difference. For example, which words are most different between topic 8 and topic 1? (For those unfamiliar with the math, just know that the negative values are terms belonging to topic 1 and the positive values belong to topic 8.)


```{r comparing topics, echo = FALSE, results = "show"}

fed_topics_beta %>% 
  mutate(topic = paste0("topic", topic)) %>% 
  tidyr::pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic8 > 0.001 |topic1 > 0.001) %>% 
  mutate(log_ratio = log(topic8 / topic1)) %>% 
  filter(abs(log_ratio) > 10) %>% 
  mutate(bar = ifelse(log_ratio < 0, "topic1", "topic8")) %>% 
  ggplot(aes(x = reorder(x = term, log_ratio), y = log_ratio, fill = bar)) +
  geom_col() +
  ylab("Log2 ratio of beta in topic 8 / topic 1") +
  xlab(NULL) +
  ggtitle(NULL) +
  theme_bw() +
  coord_flip()

```

If nothing else, we are all familiar with Topic 1...

$~$

In addition to finding which words belong to which topics, we can also determine which topics belong to which documents. Since we know roughly when Topic 1 is identified, we can confirm our observations and determine just when each topic is most likely to occur.

```{r topics by date, echo = FALSE, results = "show"}

fed_topics_gamma <- tidytext::tidy(fed_lda, matrix = "gamma")

fed_topics_gamma %>% 
  mutate(document = lubridate::as_date(stringr::str_sub(document, start = 12))) %>% 
  mutate(topic = as.character(topic), topic = paste0("Topic ", topic)) %>% 
  ggplot(aes(document, gamma)) +
  geom_line(size = 1.02, show.legend = F) +
  facet_wrap(~topic, ncol = 2) +
  xlab("Date") +
  ylab("gamma") +
  ggtitle("Probability Density Functions of Topics") +
  theme_bw()


```

Topic 2 appears in the first meeting of each year. There are 12 peaks in the sample period and 12 years of data. The meetings are not on January first, remember, and the vertical time bars on the charts have intervals of 2.5 years. There is a section at the beginning of the first FOMC Minutes release of each year called “Annual Organizational Matters” (see [this FOMC Minutes release](https://www.federalreserve.gov/monetarypolicy/fomcminutes20200129.htm), for example) which only appears in first-meeting-of-the-year meetings. People are elected to their positions at the Fed (or are they appointed? I didn’t vote for them...). Topic 2 is the Fed's version of housekeeping.

$~$

In the event you did not believe me, the probability of Topic 1 appearing in FOMC Minutes shoots up into the sky during the 2020 meetings.


## References

Board of Governors of the Federal Reserve System (US), 10-Year Treasury Constant Maturity Rate [DGS10], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/DGS10, January 7, 2021.

Loughran, T. and McDonald, B. (2016). Textual analysis in accounting and finance: A survey. Journal of Accounting Research 54(4), 1187-1230. doi: 10.2139/ssrn.2504147 https://sraf.nd.edu/textual-analysis/resources/#Master%20Dictionary.