---
title: "Minutes Analysis"
author: "Connor Krenzer"
date: "1/4/2021"
output:
  pdf_document: default
  html_document: default
---
## Introduction
The Federal Reserve is the central bank of the United States. Central banks conduct a country's monetary policy (policy pertaining to money--interest rates, regulation of the banking sector, etc.) in the aims of achieving it's 'dual mandate' of full employment (loosely defined as "anyone who wants a job can get a job") and price stability (predictable price changes throughout the United States as a whole). You can read more about what the Fed is and what they do [on their website](https://www.federalreserve.gov/faqs/about_12594.htm). For all of you Fed haters out there, perhaps [one of these articles](https://mises.org/topics/fed) aligns more closely with your views.





Every six weeks, policy makers on the Federal Reserve's Federal Open Market Committee (FOMC) meet to discuss economic indicators to determine and vote on policy in response to these indicators, among other things. The FOMC Statement released following this meeting can send the entire economy into turmoil--so much so that the minutes are withheld from the public for three weeks before becoming available.





The data in this analysis includes all FOMC Minutes between January 28th, 2009 to November 5th, 2020 (inclusive). As of the time of writing, the Minutes for the December 15th-16th meeting have not been released.

$~$

I will explore these texts using various text mining approaches including sentiment analysis, tf-idf (term frequency * inverse document frequency), and topic modeling.


Should you want to try running this code on your own computer, know that it will take more than a few hours to run!


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      results = "hide",
                      error = F,
                      warning = F,
                      message = F)
```

## Packages

Note that I am using the pacman package here, because it incorporates both the installation and loading of packages quite nicely.

I recommend pulling the information using the HTML version of these meetings, as the PDFs are a nightmare to clean (I'm speaking from experience). If you do decide to read in the pdf text into R, I suggest using the extract_text() function instead of the tabulizer package because it is smart enough to recognize columns of selectable PDF text as opposed to only individual lines.

```{r, echo = TRUE}
if(!require(pacman)) install.packages("pacman")
pacman::p_load(tidytext, dplyr, stringr, rvest)

```

## Data Import
Describe how you will import your data and how you will organize it--import the obama-era data, add an identifier, such as "Obama" column, a date column, then do the same for the trump era.


Since I started this process using the pdf files, I will use their file names to extract the date (which correspond perfectly to the different part of the url online)
```{r, echo = TRUE}
# Add file imports here

# Example file extraction
files <- paste("Fed Minutes Releases/Sample/",
               list.files("Fed Minutes Releases/Sample/"),
               sep = "")

text <- character(4)
counter <- 1
for(i in files){
  text[counter] <- str_to_lower(tabulizer::extract_text(i)) 
counter <- counter + 1
  }
text[1]


```

## Preprocessing
This text is messy--the first few pages just list off the names of present members, there are loads of newline characters, page numbers, underscores making horizontal lines (------------), and several other issues. Substantial data wrangling must take place before these texts are ready for analysis.

```{r, echo = TRUE}
# Note: using paste0() to keep the code from running off the line
# (so I don't have to get into dark LaTex magic!)
str_detect(text[3],
           paste0("minutes\\s+of\\s+the\\s+meeting\\s+of\\s+january\\s+\\d.*",
                  "\\d?,\\s+\\d{4}\\s+page\\s+\\d{1,2}|minutes\\s+of\\s+the\\s+",
                  "meeting\\s+of\\s+may\\s+\\d.*\\d?,\\s+\\d{4}\\s+page\\s+\\d{1,2}")
)


```






