---
title: "Minutes Analysis"
author: "Connor Krenzer"
date: "1/4/2021"
output:
  pdf_document: default
  html_document: default
---
## Introduction
The Federal Reserve is the central bank of the United States. Central banks conduct a country's monetary policy (policy pertaining to money--interest rates, regulation of the banking sector, etc.) in the aims of achieving it's 'dual mandate' of full employment (loosely defined as "anyone who wants a job can get a job") and price stability (predictable price changes throughout the United States as a whole). You can read more about what the Fed is and what they do [on their website](https://www.federalreserve.gov/faqs/about_12594.htm). For all of you Fed haters out there, perhaps [one of these articles](https://mises.org/topics/fed) aligns more closely with your views.





Every six weeks, policy makers on the Federal Reserve's Federal Open Market Committee (FOMC) meet to discuss economic indicators to determine and vote on policy in response to these indicators, among other things. The FOMC Statement released following this meeting can send the entire economy into turmoil--so much so that the minutes are withheld from the public for three weeks before becoming available.





The data in this analysis includes all FOMC Minutes between January 28th, 2009 to November 5th, 2020 (inclusive). As of the time of writing, the Minutes for the December 15th-16th meeting have not been released.

$~$

I will explore these texts using various text mining approaches including sentiment analysis, tf-idf (term frequency * inverse document frequency), and topic modeling.


Should you want to try running this code on your own computer, know that it will take more than a few hours to run! Instead, I suggest you run the code using a small sample of documents.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      results = "hide",
                      error = F,
                      warning = F,
                      message = F)
```

## Packages

Note that I am using the pacman package here, because it incorporates both the installation and loading of packages quite nicely.

I recommend pulling the information using the HTML version of these meetings. The PDFs are a nightmare to clean (speaking from experience)! If you do decide to read the pdf text into R, I suggest using the extract_text() function from the tabulizer package because it is smart enough to recognize columns of selectable PDF text. Other popular packages for reading in PDFs, such as readtext or pdftools, will only read across the page left to right, then down--despite the pages containing two columns.

```{r, echo = TRUE}
# The names of all your installed packages
package_names <- rownames(installed.packages())

# Installing packages used in this project
if(!"magrittr" %in% package_names) install.packages("magrittr")
if(!"dplyr" %in% package_names) install.packages("dplyr")
if(!"stringr" %in% package_names) install.packages("stringr")
if(!"tidytext" %in% package_names) install.packages("tidytext")
if(!"rvest" %in% package_names) install.packages("rvest")
if(!"tibble" %in% package_names) install.packages("tibble")

# Loading in dplyr because it is used too often
# to :: it the entire time
library(dplyr)


```

## Data Import

Since I started this process using the pdf files, I will use their file names to determine the corresponding url. If I had started mining this data from HTML in the first place, I might have come up with a more elegant solution, but downloading all the PDF files and using their names to put together urls works fine enough...

There are two formats for the HTML file--those the Fed released before 2012 and those released after. Therefore, two functions are used to read in the different formats.
```{r, echo = TRUE}
# The functions to read in and clean the documents:
# For the Pre-2012 releases
old_format <- function(id){
  
  # Note: using str_c() to prevent the code from going out of
  # bounds in the output (so I don't have to get into dark LaTex magic!)
  name <- stringr::str_c("https://www.federalreserve.gov/monetarypolicy/",
                id,
                ".htm")
  
  tibble(doc_id = id, text = rvest::read_html(name) %>% 
           rvest::html_nodes("#leftText") %>% 
           rvest::html_text() %>% 
           stringr::str_split("\n", simplify = T) %>% 
           as.vector() %>% 
           magrittr::extract(38:length(.)) %>% 
           magrittr::extract(!str_detect(., "_+")) %>%  # removing the signature
           stringr::str_c(., collapse = " ") %>%
           stringr::str_to_lower() %>% 
           stringr::str_replace_all("'s|s'|\"|,|:|;|-{2,}|\\s+\\.\\s+", " ") %>% 
           stringr::str_squish() %>% 
           stringr::str_remove_all("return to text[\\d]*|return to top") %>% 
           stringr::str_trim()
  ) %>% 
    return()
  
}#end of old_format()


# For releases 2012 and onward:
# We can always assume the first 37 entries are bogus
# Keep everything following the 37th entry
new_format <- function(id){
  name <- stringr::str_c("https://www.federalreserve.gov/monetarypolicy/",
                id,
                ".htm")
  
  tibble(doc_id = id, text = rvest::read_html(name) %>% 
           rvest::html_nodes("p") %>% 
           rvest::html_text() %>% 
           stringr::str_trim() %>% 
           stringr::str_squish() %>% 
           magrittr::extract(38:length(.)) %>% 
           magrittr::extract(!str_detect(., "_+")) %>%  # removing the signature
           stringr::str_c(., collapse = " ") %>%
           stringr::str_to_lower() %>% 
           stringr::str_replace_all("'s|s'|\"|,|:|;|-{2,}|\\s+\\.\\s+", " ") %>% 
           stringr::str_squish() %>% 
           stringr::str_remove_all("return to text[\\d]*|return to top") %>% 
           stringr::str_trim()
  ) %>% 
    return()
  
}#end of new_format()






# The paths to the PDF files
dirs <- c("Fed Minutes Releases/Obama Era/Pre 2012/",
          "Fed Minutes Releases/Obama Era/Post 2012/",
          "Fed Minutes Releases/Trump Era/")


# An empty tibble--document names and their corresponding
# text will be added to it.
documents <- tibble::tibble(doc_id = NULL, text = NULL)



# Reading in the old formats (pre-2012)
for(i in stringr::str_remove_all(list.files(dirs[1]), "\\.pdf")){
  documents <- bind_rows(documents, new_format(i))
  
  # simple, yet effective way of showing the operation's progress
  cat(".")
}

# Reading in the new formats (2012-present)
for(i in stringr::str_remove_all(list.files(dirs[2:3]), "\\.pdf")){
  documents <- bind_rows(documents, new_format(i))
  
  # simple, yet effective way of showing the operation's progress
  cat(".")
}

```

## The Dataset
The data is almost ready for analysis--we have two columns: one for the document name, and another for the contents of that document. If you are interested in performing a similar analysis, the data frame can now be transformed in a number of ways. You can use the unnest_tokens() function from the tidytext package to extract tokens (words, groups of words, paragraphs, etc.) and follow perform sentiment analysis, or you could make a term-document matrix and perform topic modeling. We will do both here, but let's begin by taking a look at the dataset.

```{r, echo = TRUE}

knitr::kable(documents)

```


As we can see...



