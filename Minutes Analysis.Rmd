---
title: "Minutes Analysis"
author: "Connor Krenzer"
date: "1/4/2021"
output:
  pdf_document: default
  html_document: default
---
## Introduction
The Federal Reserve is the central bank of the United States. Central banks conduct a country's monetary policy (policy pertaining to money--interest rates, regulation of the banking sector, etc.) in the aims of achieving it's 'dual mandate' of full employment (loosely defined as "anyone who wants a job can get a job") and price stability (predictable price changes throughout the United States as a whole). You can read more about what the Fed is and what they do [on their website](https://www.federalreserve.gov/faqs/about_12594.htm). For all of you Fed haters out there, thinking that I'm misleading people by linking to the Fed's website, do not disparage! Perhaps [one of these articles](https://mises.org/topics/fed) is more your style.





Every six weeks, policy makers on the Federal Reserve's Federal Open Market Committee (FOMC) meet to discuss economic indicators to determine and vote on policy in response to these indicators, among other things. The FOMC Statement released following this meeting can send the entire economy into turmoil--so much so that the minutes are withheld from the public for three weeks before becoming available.





The data in this analysis includes all FOMC Minutes between January 28th, 2009 to November 5th, 2020 (inclusive). As of the time of writing, the Minutes for the December 15th-16th meeting have not been released.

$~$

I will explore these texts using various text mining approaches including sentiment analysis, tf-idf, and topic modeling.


Should you want to try running this code on your own computer, know that it will take more than a few hours to run! Instead, I suggest you run the code using a small sample of documents.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      results = "hide",
                      error = F,
                      warning = F,
                      message = F)
```

## Packages

I recommend pulling the information using the HTML version of these meetings. The PDFs are a nightmare to clean (speaking from experience)! If you do decide to read the pdf text into R, I suggest using the extract_text() function from the tabulizer package because it is smart enough to recognize columns of selectable PDF text. Other popular packages for reading in PDFs, such as readtext or pdftools, will only read across the page left to right, then down--despite the pages containing two separate columns of text.

```{r, echo = TRUE}
# The names of all your installed packages
package_names <- rownames(installed.packages())

# Installing packages used in this project
if(!"dplyr" %in% package_names) install.packages("dplyr")
if(!"stringr" %in% package_names) install.packages("stringr")
if(!"tidytext" %in% package_names) install.packages("tidytext")
if(!"rvest" %in% package_names) install.packages("rvest")
if(!"tidyr" %in% package_names) install.packages("tidyr")
if(!"ggplot2" %in% package_names) install.packages("ggplot2")

# Loading in dplyr because it is used too often
# to :: it the entire time
library(dplyr)

# Packages which are difficult to use without loading:
library(rvest)
library(ggplot2)


```

## Data Import

Since I started this process using the pdf files, I will use their file names to determine the corresponding url. If I had started mining this data from HTML in the first place, a more elegant solution might have been used, but downloading all the PDF files and using their names to put together urls works fine enough...

There are two formats for the HTML file--one for the Fed Minutes before 2012 and those released on or after that year. With this in mind, two functions will be used to read in the different formats. The documents are parsed in chronological order but are also arranged after being added to the tibble, in the event that something were to happen to the order of the files in the "Fed Minutes Releases" folder from which these names were pulled.
```{r, echo = TRUE}
old_format <- function(id){
  
  # Note: using str_c() to prevent the code from going out of
  # bounds in the output (so I don't have to get into dark LaTex magic!)
  name <- stringr::str_c("https://www.federalreserve.gov/monetarypolicy/",
                         id,
                         ".htm")
  
  tibble(doc_id = id, text = read_html(name) %>% 
           html_nodes("#leftText") %>% 
           html_text() %>% 
           stringr::str_split("\n", simplify = T) %>% 
           as.vector() %>% 
           stringr::str_to_lower() %>% 
           stringr::str_replace_all("'s|\"|,|:|;|-{2,}", " ") %>% 
           magrittr::extract(!stringr::str_detect(., "_+")) %>%  # removing the signature
           stringr::str_c(., collapse = " ") %>%
           stringr::str_replace_all("mr\\.{1}|mrs\\.{1}|ms\\.{1}", " ") %>% 
           stringr::str_replace_all("\\d", " ") %>% 
           stringr::str_squish() %>% 
           stringr::str_remove_all("return to text[\\d]*|return to top") %>% 
           stringr::str_trim()
  ) %>% 
    return()
  
}#end of old_format()


# For releases 2012 and onward:
# We can always assume the first 37 entries are bogus
# Keep everything following the 37th entry
new_format <- function(id){
  name <- stringr::str_c("https://www.federalreserve.gov/monetarypolicy/",
                         id,
                         ".htm")
  
  tibble(doc_id = id, text = read_html(name) %>% 
           html_nodes("p") %>% 
           html_text() %>% 
           stringr::str_trim() %>% 
           stringr::str_squish() %>% 
           magrittr::extract(38:length(.)) %>% 
           magrittr::extract(!stringr::str_detect(., "_+")) %>%  # removing the signature
           stringr::str_to_lower() %>% 
           stringr::str_replace_all("'s|\"|,|:|;|-{2,}", " ") %>% 
           stringr::str_c(., collapse = " ") %>%
           stringr::str_replace_all("mr\\.{1}|mrs\\.{1}|ms\\.{1}", " ") %>%
           stringr::str_replace_all("\\d", " ") %>% 
           stringr::str_squish() %>% 
           stringr::str_replace_all("return to text[\\d]*|return to top", " ") %>% 
           stringr::str_trim()
  ) %>% 
    return()
  
}#end of new_format()






# The paths to the PDF files
dirs <- c("Fed Minutes Releases/Obama Era/Pre 2012/",
          "Fed Minutes Releases/Obama Era/Post 2012/",
          "Fed Minutes Releases/Trump Era/")


# An empty tibble--document names and their corresponding
# text will be added to it.
documents <- tibble::tibble(doc_id = NULL, text = NULL)



# Reading in the old formats (pre-2012)
for(i in stringr::str_remove_all(list.files(dirs[1]), "\\.pdf")){
  documents <- bind_rows(documents, old_format(i))
  
  # simple yet effective way of showing the operation's progress
  cat(".")
}

# Reading in the new formats (2012-present)
for(i in stringr::str_remove_all(list.files(dirs[2:3]), "\\.pdf")){
  documents <- bind_rows(documents, new_format(i))
  
  # simple yet effective way of showing the operation's progress
  cat(".")
}

# Arranging the documents in chronological order
documents <- arrange(documents, doc_id)

```

## The Dataset
The data is almost ready for analysis. We now have two columns: one for the document name, and another for the contents of that document. If you are interested in performing a similar analysis, the data frame can now be transformed in a number of ways. You can use the unnest_tokens() function from the tidytext package to extract tokens (words, groups of words, paragraphs, etc.) and perform sentiment analysis, or you could make a term-document matrix and perform topic modeling.

We will do both, but let's begin by taking a look at the dataset:


> `r stringr::str_sub(documents$text[1], start = 8489, end = 8802)`


The quote above is an excerpt from the Minutes release for the meeting dated January 28th, 2009. You might notice that some words have parsing irregularities, such as the lack of space between characters on the parentheses surrounding "millions of dollars equivalent." This section of text was chosen to represent the 'worst-case-scenario' for parsing these texts. No algorithm is perfect, but one can see these errors are infrequent and therefore unlikely to heavily skew the results of the analysis. The text details the relations with the Bank of Canada and the Bank of Mexico--tables typically have issues parsing.

$~$

A great place to start any textual analysis is with word counts.



## Common Words

Brian F. Madigan is the Secretary who writes all these meetings--he has been the secretary for at least the last 11 years! In a way, we are exploring one man's work over the course of a decade...

After removing 'stop words'--those words that are extremely common and provide very little meaning (Ex. "the," "of," "a," "to," etc.)--which words does Brian use most frequently?

```{r, echo = FALSE, results = "show"}
unigram_unnested_documents <- documents %>% 
  tidytext::unnest_tokens(word, text)



head(unigram_unnested_documents %>% 
       anti_join(tidytext::stop_words) %>% 
       count(word, sort = T), 10) %>% 
  ggplot(aes(x = reorder(word, n), y = n, fill = n)) +
  geom_col(show.legend = F) +
  ylab("number of occurrences") +
  ggtitle("Word Counts Across All Documents") +
  coord_flip() +
  theme_bw()

```

These are words one expects to find in official banking documents; the terms "committee," "inflation," and "market" dominate across all `r nrow(documents)` minutes. These words may be insightful for some people, but most people would agree that most of these terms are uninteresting finds. Much of what Brian wrote is generic banking lingo from which we cannot derive much meaning. We can still use these terms to draw interesting conclusions with these terms. We will visualize their usage over time, for example, but we first want to know which terms are unique to different documents.

To extract words that have more significance in each document, we can add a weighting factor to the terms with the tf-idf statistic.








## TF-IDF

The tf-idf statistic is designed to reduce the importance of words that occur often across documents. The math behind it is based on the natural logarithm. It is the product of the 'term frequency' and 'inverse document frequency:

_Term Frequency = (# of Occurrences in Document) / (# of Unique Words in Document)_

_Inverse Document Frequency = ln((Total # of Docs in Corpus) / (# of Docs where Term Appears))_

Therefore...
__tf-df = (tf * idf)__


$~$

You can think of tf-idf as a formula that finds terms that occur frequently--but not too frequently. Let's see which words are most important among all Fed Minutes releases, as measured by the tf-idf:
```{r echo = FALSE, results = "show"}

knitr::kable(head(unigram_unnested_documents %>% 
                    count(doc_id, word, sort = T) %>%
                    ungroup() %>% 
                    tidytext::bind_tf_idf(word, doc_id, n) %>% 
                    arrange(desc(tf_idf)), 10))

```


I'm tired of seeing those words everywhere. The correct interpretation of this table, however, is that the words "pandemic," "coronavirus," and "outbreak" are the most important terms, as determined by the tf-idf. Let's filter out the words "pandemic," "coronavirus," "virus," and "outbreak" to see what else tf-idf deems important. Since we all know when hurricane season rolls around, let's remove the term "hurricane" as well. Finally, after some inspection, the names of the months, "shall," and "our" should also be filtered out.

Let's see a plot of these terms after removing these terms:

```{r echo = FALSE, results = "show"}

unigram_unnested_documents %>% 
  count(doc_id, word, sort = T) %>%
  ungroup() %>% 
  filter(!word %in% c("pandemic", "coronavirus", "virus",
                      "outbreak", "our", "shall",
                      "hurricane", "hurricanes",
                      stringr::str_to_lower(month.name))) %>% 
  tidytext::bind_tf_idf(word, doc_id, n) %>% 
  arrange(desc(tf_idf)) %>% 
  slice(1:10) %>% 
  ggplot(aes(x = reorder(word, tf_idf), y = tf_idf, fill = doc_id)) +
  geom_col(show.legend = F) +
  xlab(NULL) +
  ylab("tf-idf") +
  ggtitle("TF-IDF Statistic for Fed Minutes") +
  coord_flip() +
  theme_bw()

```


Now that's something interesting! The Fed uses many different acronyms to describe its programs. To name a few, "yct" is the "yield control target," talf is short for "term asset-backed securities loan facilities," and "elb" stands for "effective lower bound."

$~$

You may be wondering how these terms relate to the current events at the time. Yield curve controls (yield curve targets, YCT), for instance, were a hot topic in June 2020, when the Fed was evaluating the experiences central banks across the developed world had with controlling government bond yields in response to the recession. The second phrase, term asset-backed securities loan facilities (talf), was a major policy the Fed pursued in the aftermath of the Great Recession, which is also consistent with these findings.

Clearly, then, we can see the value brought from this statistic. It allows us to identify important aspects of documents to hone in on topics of interest.













## Change Over Time

We can use the information from the tf-idf to identify changes in a term's use over time, using an index with the first observation

```{r, echo = FALSE, results = "show"}

change_over_time <- function(data, word1, word2, graph_title){

  graph_title <-  paste("Usage of ",
                       stringr::str_to_title(word1),
                       " vs. ",
                       stringr::str_to_title(word2),
                       " Over Time")
  
data %>% 
  count(doc_id, word) %>% 
  ungroup() %>% 
  filter(word %in% c(word1, word2)) %>% 
  mutate(doc_id = lubridate::as_date(stringr::str_sub(doc_id, start = 12))) %>% 
  tidyr::pivot_wider(names_from = word, values_from = n) %>% 
  mutate(.[, 2] / unlist(.[49, 2]),
         .[, 3] / unlist(.[49, 3])) %>% 
  tidyr::pivot_longer(names_to = "word", values_to = "index", cols = 2:3) %>% 
  ggplot(aes(x = doc_id, y = index, color = word)) +
  geom_line(show.legend = T) +
  xlab("FOMC Meetings") +
  ylab("Uses in Minutes Releases, Index (Base: 01-28-15)") +
  ggtitle(graph_title)

}#end of change_over_time()


change_over_time(data = unigram_unnested_documents,
                 word1 = "inflation",
                 word2 = "unemployment")

  
```


TASKS:
1. parse the date from the doc_id column
2. Use line graphs to plot usage of various words over time (Ex. inflation and unemployment, or some other interesting plots)





find the number of occurrences of these words in each document over time. Does it change? This would be a line chart.


I think the way to go about this is to count word usage by document, then filter, then plot..?


As we can see, ... changes with time. Can we use sentiment analysis to validate these findings further?































## REPEAT THE PREVIOUS THREE SECTIONS WITH NGRAMS!!!

Also give the tables titles--and make them look better!




















## Markov Chains

Make Markov chains with your n-grams










## Sentiment Analysis


add a nice transition, too.


this is the cool part---you break the terms into their ngrams, then plot them over time!

you can simply use parse_number() to retrieve the dates

Talk about the packages, sentiment lexicons, why some are more suited for certain cases than others, etc.

Your readthrough notes say "n-grams are good because they provide context and let you adjust for negation words, but their downside is that the number of observations decreases significantly"--->They're better suited for large documents (which suits this project perfectly)




The loughran sentiment lexicon can be used to analyze this data. An ngram approach will be used because many terms will be negated












## Topic Modeling


```{r,echo=F}






```



echo = FALSE hides the code, but will evaluate it and show its output in the knit.

include = FALSE hides the code AND the output from the knit, but will evaluate the code silently.















throw a cor.test() in there somewhere...


